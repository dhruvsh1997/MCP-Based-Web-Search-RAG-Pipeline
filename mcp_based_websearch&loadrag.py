# -*- coding: utf-8 -*-
"""MCP-Based-WebSearch&LoadRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16iIIfBvG9f5ZbAfa4UWK1V2wEGoFDnLb
"""

# Step 0: install
!pip install --upgrade -q langchain langchain-community langchain-openai langchain-mcp-adapters faiss-cpu beautifulsoup4 unstructured sentence-transformers aiohttp pyngrok

import os
import json
import asyncio
from aiohttp import web
from google.colab import userdata
from pyngrok import ngrok
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_community.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from tqdm import tqdm
from typing import List, Dict, Any
from dataclasses import dataclass
import aiohttp

# Set API keys and ngrok authtoken
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY4')
os.environ["SERPER_API_KEY"] = userdata.get('SERPER_API_KEY')
os.environ["NGROK_AUTH_TOKEN"] = userdata.get('NGROK_TOKEN')

# Define the Google Serper tool logic
async def google_serper_search(query: str) -> List[Dict[str, str]]:
    try:
        search = GoogleSerperAPIWrapper(k=10, type="search")
        # Use results method for site-specific searches
        results = search.results(query)
        organic = results.get("organic", [])
        return [{"title": item.get("title", ""), "link": item.get("link", "")} for item in organic[:10]]
    except Exception as e:
        return [{"error": f"Search failed: {str(e)}"}]

# import asyncio
# async def test_search_function():
#     test_urls = [
#         'aalas.org: How to prevent Rabies?',
#         'jstor.org: How to prevent Rabies?',
#         'avmajournals.avma.org: How to prevent Rabies?',
#         'animalmicrobiome.biomedcentral.com: How to prevent Rabies?',
#         'irishvetjournal.biomedcentral.com: How to prevent Rabies?',
#         'bmcvetres.biomedcentral.com: How to prevent Rabies?',
#         'animaldiseases.biomedcentral.com: How to prevent Rabies?',
#         'actavetscand.biomedcentral.com: How to prevent Rabies?',
#         'sciendo.com: How to prevent Rabies?',
#         'canadianveterinarians.net: How to prevent Rabies?',
#         'parasite-journal.org: How to prevent Rabies?',
#         'sciencedirect.com: How to prevent Rabies?',
#         'microbiologyresearch.org: How to prevent Rabies?',
#         'jvas.in: How to prevent Rabies?',
#     ]

#     print("Testing google_serper_search function:")
#     for url_query in test_urls:
#         print(f"\nSearching for: {url_query}")
#         try:
#             # Assuming google_serper_search is defined in a previous cell and is accessible
#             results = await google_serper_search(url_query)
#             print("Results:")
#             for item in results:
#                 print(f"  Title: {item.get('title', 'N/A')}")
#                 print(f"  Link: {item.get('link', 'N/A')}")
#                 if "error" in item:
#                     print(f"  Error: {item['error']}")
#         except Exception as e:
#             print(f"An error occurred during search for {url_query}: {e}")

# # Run the test function
# await test_search_function()

# MCP server routes
async def handle_jsonrpc(request):
    try:
        data = await request.json()
        method = data.get('method')
        request_id = data.get('id')

        if method == 'get_tool_schema':
            schema = {
                "name": "google_serper_search",
                "description": "Search the web using Google Serper API and return relevant URLs.",
                "parameters": {
                    "type": "object",
                    "properties": {"query": {"type": "string"}},
                    "required": ["query"]
                }
            }
            response = {"jsonrpc": "2.0", "result": schema, "id": request_id}
        elif method == 'google_serper_search':
            query = data.get('params', {}).get('query', '')
            results = await google_serper_search(query)
            response = {"jsonrpc": "2.0", "result": results, "id": request_id}
        else:
            response = {"jsonrpc": "2.0", "error": {"code": -32601, "message": "Method not found"}, "id": request_id}

        return web.json_response(response)
    except json.JSONDecodeError:
        return web.json_response(
            {"jsonrpc": "2.0", "error": {"code": -32700, "message": "Parse error"}, "id": None}, # Use None instead of null
            status=400
        )

async def handle_get(request):
    return web.Response(text="MCP server: Use POST for JSON-RPC requests")

# Start the aiohttp server
from aiohttp import web
app = web.Application()
app.add_routes([
    web.get('/', handle_get),
    web.post('/', handle_jsonrpc)
])

async def start_server():
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 5555)
    await site.start()
    print(f"MCP server running on port 5555")

# Run the server in a background task
asyncio.create_task(start_server())

# Wait for server to initialize
async def wait_for_server():
    await asyncio.sleep(5)

await wait_for_server() # Call with await

# Expose the server via ngrok
ngrok.set_auth_token(os.environ["NGROK_AUTH_TOKEN"])
http_tunnel = ngrok.connect(5555, bind_tls=True)
public_url = http_tunnel.public_url
print(f"MCP server exposed at: {public_url}")

# Simple tool class to mimic LangChain tool
@dataclass
class SimpleTool:
    name: str
    description: str
    parameters: Dict[str, Any]
    invoke_func: callable

    async def ainvoke(self, input_dict: Dict[str, Any]) -> Any:
        return await self.invoke_func(input_dict.get("query", ""))

# Custom HTTP client for JSON-RPC
async def send_jsonrpc_request(url: str, method: str, params: Dict = None, request_id: int = 1) -> Dict:
    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}
    payload = {
        "jsonrpc": "2.0",
        "method": method,
        "id": request_id
    }
    if params:
        payload["params"] = params

    async with aiohttp.ClientSession() as session:
        for attempt in range(3):
            try:
                async with session.post(url, json=payload, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        if "error" in data:
                            raise Exception(f"JSON-RPC error: {data['error']}")
                        return data.get("result")
                    else:
                        print(f"Attempt {attempt + 1} failed: HTTP {response.status}")
                        if attempt < 2:
                            await asyncio.sleep(3)
            except Exception as e:
                print(f"Attempt {attempt + 1} failed: {e}")
                if attempt < 2:
                    await asyncio.sleep(3)
        raise Exception(f"Failed to send JSON-RPC request after 3 attempts")

# Load and test MCP tools
async def load_mcp_tools():
    try:
        schema = await send_jsonrpc_request(f"{public_url}", "get_tool_schema")
        print(f"Loaded tool schema: {schema}")
        tool = SimpleTool(
            name=schema["name"],
            description=schema["description"],
            parameters=schema["parameters"],
            invoke_func=lambda query: send_jsonrpc_request(f"{public_url}", "google_serper_search", {"query": query})
        )
        return [tool]
    except Exception as e:
        print(f"Failed to load MCP tools: {e}")
        return []

# tools = asyncio.run(load_mcp_tools()) # Commented out asyncio.run
# print(f"Loaded tools: {[tool.name for tool in tools]}")

async def runGSC():
    global tools
    tools = await load_mcp_tools()
    print(f"Loaded tools: {[tool.name for tool in tools]}")
toals=None
await runGSC()

# Base URLs for filtering
base_urls = [
    "aalas.org", "jstor.org", "avmajournals.avma.org", "animalmicrobiome.biomedcentral.com",
    "irishvetjournal.biomedcentral.com", "bmcvetres.biomedcentral.com", "animaldiseases.biomedcentral.com",
    "actavetscand.biomedcentral.com", "sciendo.com", "canadianveterinarians.net", "parasite-journal.org",
    "sciencedirect.com", "microbiologyresearch.org", "jvas.in", "vaajournal.org", "frontiersin.org",
    "lp.thieme.de", "mdpi.com", "vetsci.org", "academic.oup.com", "journals.sagepub.com", "link.springer.com",
    "tandfonline.com", "onlinelibrary.wiley.com", "efsa.onlinelibrary.wiley.com",
    "resjournals.onlinelibrary.wiley.com", "beva.onlinelibrary.wiley.com", "bvajournals.onlinelibrary.wiley.com"
]

# Search for relevant URLs using MCP tool
question = "How to prevent Rabies?"
combined_queries = [f"{url}: {question}" for url in base_urls]
combined_queries

article_urls = set()
results = None

for query in tqdm(combined_queries, desc="Searching URLs"):

    if tools:
        async def querySrc():
            global results
            results = await tools[0].ainvoke({"query": query})  # Use await directly
        await querySrc()
    else:
        async def querySrc():
            global results
            results = await google_serper_search(query)  # Use await directly
        await querySrc()

    for item in results:
        if "error" not in item and any(base in item["link"] for base in base_urls):
            article_urls.add(item["link"])

urls = list(article_urls)[:20]
print(f"Total unique article URLs fetched: {len(urls)}")

# Load documents
loader = UnstructuredURLLoader(urls=urls)
documents = loader.load()

# Split documents
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
print(f"Total chunks: {len(chunks)}")

# Create vector store
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embeddings)

# Create retriever
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"lambda_mult": 0.3})

# Define the RAG prompt
rag_prompt = ChatPromptTemplate.from_template(
    """Use the following context and search results to answer the question:

Context: {context}

Search Results: {search_results}

Question: {question}

Answer concisely and accurately."""
)

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Use MCP tool or fallback
async def search_tool_func(query):
    if tools:
        return await google_serper_search({"query": query})#tools[0].ainvoke
    return await google_serper_search(query)

search_tool = lambda x: asyncio.run(search_tool_func(x))

from langchain_core.runnables import RunnableLambda

# Define the RAG chain
rag_chain = (
    {
        "context": retriever | (lambda docs: "\n".join([doc.page_content for doc in docs])),
        "search_results": RunnableLambda(search_tool) | (lambda results: "\n".join([f"{r['title']}: {r['link']}" for r in results if "error" not in r])),
        "question": RunnablePassthrough()
    }
    | rag_prompt
    | llm
    | StrOutputParser()
)

# Test the RAG chain
async def test_rag():
    result = await rag_chain.ainvoke(question)
    print("Answer:\n", result)

# Run the test
async def main_test_rag():
    try:
        await test_rag()
    finally:
        # Assuming public_url is defined and accessible
        ngrok.disconnect(public_url)

await main_test_rag()













